{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "ec58202f-3dde-4483-b569-f93f1ab4a8c7",
          "showTitle": false,
          "title": ""
        },
        "id": "GMmDTQ-NTxLJ"
      },
      "source": [
        "# Matrix Factorization\n",
        "\n",
        "We will experiment with the recent MovieLens 25M Dataset and build a recommender system using two approaches:\n",
        "* Factorizing the user-item matrix using Spark ALS implementation\n",
        "* Factorizing the item-item PMI maatrix using randomized SVD\n",
        "\n",
        "In both settings we will index the item embeddings and inspect their quality using KNN queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWLRqi0eUEzR"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark==3.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMdSQagmUGR0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXRsixXbUIOQ"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import pyspark.sql.functions as F\n",
        "sf = F\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "ss = spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aV0-pCxUK7k"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "930121a6-31fe-4b95-8459-4fb22caf15e5",
          "showTitle": false,
          "title": ""
        },
        "id": "tv27aLupTxLM"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "f2fa6086-66c5-4421-997c-3abc0bc79ab2",
          "showTitle": false,
          "title": ""
        },
        "id": "lTvnLlj6TxLN"
      },
      "outputs": [],
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
        "!unzip ml-25m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "83dc87e1-4575-41fb-b9b9-53515664b2ac",
          "showTitle": false,
          "title": ""
        },
        "id": "90rEnv40TxLP"
      },
      "source": [
        "### Loading the ratings dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "30cde39c-8089-4c5c-9b95-347a8b3414b0",
          "showTitle": false,
          "title": ""
        },
        "id": "DfsZyN9fTxLQ"
      },
      "outputs": [],
      "source": [
        "movies_df = spark.read.csv('ml-25m/movies.csv', header=True, inferSchema=True).cache()\n",
        "ratings_df = spark.read.csv('ml-25m/ratings.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Useful imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import panda as pd\n",
        "import pyspark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "63c24520-7d86-48bc-8abe-c38f26bf1bc6",
          "showTitle": false,
          "title": ""
        },
        "id": "Ol1DzytYTxLL"
      },
      "source": [
        "# Part 1 : Alternating least squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "9638b534-2471-4f74-8c9c-df471a6dbc43",
          "showTitle": false,
          "title": ""
        },
        "id": "OuXpc2bJTxLR"
      },
      "source": [
        "### Split the dataset\n",
        "We want to randomly split the dataset into train and test parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "099d08ee-00b8-496e-a91e-240c18f31a5d",
          "showTitle": false,
          "title": ""
        },
        "id": "GpxyW9KVTxLR"
      },
      "outputs": [],
      "source": [
        "training_df, validation_df = #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "b7b13148-8e7e-46f1-b856-3727dd2c7021",
          "showTitle": false,
          "title": ""
        },
        "id": "hUHA1E9TTxLS"
      },
      "source": [
        "### Build ALS model\n",
        "Using the Spark ALS implementation described here https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
        "Build a model using the ml-25m dataset.\n",
        "\n",
        "How long does the training take, change the rank (i.e. the dimension of the vectors) from 10 to 20. How does that affect training speed ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "b3fd44e1-0529-4af3-919f-3118b6e30d13",
          "showTitle": false,
          "title": ""
        },
        "id": "-pRBEPDnTxLT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3342dd06-1737-43ec-bef3-c185c601bb24",
          "showTitle": false,
          "title": ""
        },
        "id": "SaxCKf5BTxLT"
      },
      "source": [
        "### Evaluation\n",
        "Using the code described in the Spark documentation, evaluate how good your model is doing on the test set.\n",
        "The goal is to predict the held out ratings.\n",
        "A good metric could be RMSE or MAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "49bbd4e6-7c06-4929-9c97-cd56f7c7bcda",
          "showTitle": false,
          "title": ""
        },
        "id": "Uh7_BJn0TxLU"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "9de1257d-ea6d-4833-ac47-79311f0c0f8a",
          "showTitle": false,
          "title": ""
        },
        "id": "chvFoB-pTxLU"
      },
      "source": [
        "### Inspecting the results\n",
        "\n",
        "Retrieve the movie vectors from the learned model object (the property is called itemFactors) and create a spark dataset `[title, feature]` by joining with the `movies_df` dataset in the movie id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "7d9e5482-54ff-4950-9b8d-4d889c64dd7b",
          "showTitle": false,
          "title": ""
        },
        "id": "HwvK2FPpTxLV"
      },
      "outputs": [],
      "source": [
        "movie_vectors_df = #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "title_vector_array = movie_vectors_df.collect()\n",
        "titles = [r['title'] for r in title_vector_array]\n",
        "vectors = np.array([r['features'] for r in title_vector_array])\n",
        "normalized_vectors = vectors/(LA.norm(vectors, axis=1)[:, np.newaxis])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "89e7fa17-0f14-42ae-9bd7-7894dbbc6069",
          "showTitle": false,
          "title": ""
        },
        "id": "-L4K3IX6TxLV"
      },
      "source": [
        "### Using Nearest neighbours\n",
        "\n",
        "Pick a few movies, and for each of them, find-out the top 5 nearest neighbours. Make sure your KNN algorithm is fast enough. Try to understand why some results are not so good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "acefd735-d0a6-454e-811a-8ccf54092731",
          "showTitle": false,
          "title": ""
        },
        "id": "Z6P14MCdTxLW"
      },
      "outputs": [],
      "source": [
        "def knn(query_vec: np.ndarray, k:int, titles:list[str], vectors:np.ndarray) -> list[tuple[str, float]]:\n",
        "    \"\"\" Perform KNN algorithm on an array of vectors \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    query_vec: np.ndarray, shape = (d,)\n",
        "        Input query vector\n",
        "    k: int\n",
        "        nb neighbors to query\n",
        "    titles: list[str]\n",
        "        list of movies titles\n",
        "    vectors: np.ndarray\n",
        "        Array of embeddings, in the same order as the title list\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    list of top k (title, affinity score) sorted by affinity score\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GOLDEN_EYE_ID = 0\n",
        "\n",
        "def analyze(i:int):\n",
        "  print(f\"Query title : {titles[i]}\")\n",
        "  query_vec = normalized_vectors[i]\n",
        "  ret = knn(query_vec, 10, titles, normalized_vectors)\n",
        "  for res in ret:\n",
        "    print(res)\n",
        "\n",
        "analyze(GOLDEN_EYE_ID)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "632c1d5f-eae1-4c24-b710-a1243abb2822",
          "showTitle": false,
          "title": ""
        },
        "id": "Bb2a62-OTxLX"
      },
      "source": [
        "# Part 2 : PMI and RSVD"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We now are going to factorize the item-item PMI matrix using randomized SVD."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Size estimation\n",
        "\n",
        "Let's first estimate the size of the matrix we are about to build.\n",
        "\n",
        "Reminder : we will generate the co-occurence matrix $C$ from all the pairs of\n",
        "movies that we find in users ratings. \n",
        "This matrix can be big.\n",
        "\n",
        "Namely, if a user has rated movies `(1, 2, 3, 4)`, we will generate 6 pairs :\n",
        "`(1, 2), (1, 3), (2, 3), (1, 4), (2, 4), (3, 4)`.\n",
        "\n",
        "Formally, if a user has rated $n$ movies, he will generate $n \\cdot (n - 1) \\; / \\; 2$ pairs. We should be careful of users that have rated a lot of movies.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create function to compute the number of pair generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def number_of_pairs_to_be_generated(ratings_df: pyspark.sql.DataFrame) -> int:\n",
        "  return #TODO\n",
        "\n",
        "print(\n",
        "    f\"Number of positive ratings : {ratings_df.count():,}\"\n",
        "    f\", that should generate {number_of_pairs_to_be_generated(ratings_df):,} pairs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keep positive interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first things first we only keep movies liked by user.\n",
        "positive_ratings_df = #TODO\n",
        "\n",
        "# positive_ratings_df Schema\n",
        "# root\n",
        "#  |-- userId: integer (nullable = true)\n",
        "#  |-- movieId: integer (nullable = true)\n",
        "#  |-- rating: double (nullable = true)\n",
        "#  |-- timestamp: integer (nullable = true)\n",
        "\n",
        "print(\n",
        "    f\"Number of positive ratings : {positive_ratings_df.count():,}\"\n",
        "    f\", that should generate {number_of_pairs_to_be_generated(positive_ratings_df):,} pairs\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keep meaningful movies\n",
        "\n",
        "We will keep only movies having a sufficient amount of positive ratings.\n",
        "First, that will make the computations lighter, second, it will prevent us from\n",
        "computing embeddings for movies we have very little information on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# There are not that many movies in the dataset, we can safely bring their rating counts to pandas\n",
        "movie_counts_df = #TODO\n",
        "\n",
        "# movie_counts_df Schema\n",
        "# root\n",
        "#  |-- movieId: integer (nullable = true)\n",
        "#  |-- count: integer (nullable = true)\n",
        "\n",
        "movie_counts_pdf = movie_counts_df.toPandas()\n",
        "print(f\"Number of Movies in original dataset : {len(movie_counts_pdf):,}\")\n",
        "movie_counts_pdf.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "movie_counts_pdf = #TODO\n",
        "\n",
        "# Display Number of movies per number of ratings bucket with matplotlib\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].hist(movie_counts_pdf[\"count\"], log=True, rwidth=0.9)\n",
        "axes[0].set_xlabel(\"positive ratings count\")\n",
        "axes[0].set_ylabel(\"number of movies\")\n",
        "axes[0].set_title(\"Number of movies per number of ratings bucket\")\n",
        "\n",
        "axes[1].hist(movie_counts_pdf[\"count\"][movie_counts_pdf[\"count\"] < 100], log=True, rwidth=0.9)\n",
        "axes[1].set_xlabel(\"positive ratings count\")\n",
        "axes[1].set_ylabel(\"number of movies\")\n",
        "axes[1].set_title(\"Number of movies per number of ratings bucket\\nZoom on least rated movies\")\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MIN_COUNT = #TODO\n",
        "kept_movies_df = #TODO\n",
        "\n",
        "filtered_ratings_df = positive_ratings_df.join(kept_movies_df, on=\"movieId\")\n",
        "\n",
        "print(\n",
        "    f\"Number of positive ratings : {filtered_ratings_df.count():,}\"\n",
        "    f\", that should generate {number_of_pairs_to_be_generated(filtered_ratings_df):,} pairs\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limit the number of pairs\n",
        "\n",
        "We have way less movies but still almost all ratings and a lot of pairs ! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at how many ratings are done user by user\n",
        "# When user has scored a lots of movies, the number of pairs he generates increases quadratically !\n",
        "ratings_count_by_user_df = #TODO\n",
        "\n",
        "# ratings_count_by_user_df Schema\n",
        "# root\n",
        "#  |-- userId: integer (nullable = true)\n",
        "#  |-- count: long (nullable = false)\n",
        "\n",
        "ratings_count_by_user_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will sample user ratings to limit the maximum number of ratings per user (in expectation)\n",
        "# You can use \".filter(F.rand() < (EXPECTED_MAX_USER_RATINGS / F.col('count')))\"\n",
        "EXPECTED_MAX_USER_RATINGS = 40\n",
        "\n",
        "# ratings_sampled_df Schema\n",
        "# root\n",
        "#  |-- userId: integer (nullable = true)\n",
        "#  |-- movieId: integer (nullable = true)\n",
        "\n",
        "ratings_sampled_df =  #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Number of positive ratings after sampling : {ratings_sampled_df.count():,}\"\n",
        "    f\", that should generate {number_of_pairs_to_be_generated(ratings_sampled_df):,} event pairs\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the PMI matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reminder, the PMI matrix writes\n",
        "$$\n",
        "PMI(i, j) = \\log\\left(\\frac{p(i, j)}{p(i)p(j)}\\right)\n",
        "$$\n",
        "that we estimate with\n",
        "$$\n",
        "\\widehat{PMI}(i, j) = \\log\\left(\\frac{C_{i, j} \\cdot n}{C_i \\cdot  C_j}\\right)\n",
        "$$\n",
        "where\n",
        "* $C_{i, j}$ is the number of pairs (i, j) (i.e. the number of users that have\n",
        "  given a positive feedback for both movie i and movie j.\n",
        "* $C_{i}$ is total the number of pairs containing i\n",
        "* $C_{j}$ is total the number of pairs containing j\n",
        "* $n$ is the total number of pairs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1 : compute co-occurence matrix $C_{i,j}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the coocurrence matrix from the ratings_sampled_df DataFrame\n",
        "\n",
        "# cooccurence_df Schema\n",
        "# root\n",
        "#  |-- movieId1: integer (nullable = true)\n",
        "#  |-- movieId2: integer (nullable = true)\n",
        "#  |-- pair_count: long (nullable = false)\n",
        "\n",
        "cooccurence_df = ratings_sampled_df. ... #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_generated_pairs = int(cooccurence_df.select(F.sum(\"pair_count\")).collect()[0][\"sum(pair_count)\"])\n",
        "print(f\"We have generated {total_generated_pairs:,} event pairs for {cooccurence_df.count():,} movie pairs (i, j)\")\n",
        "cooccurence_df.show(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2 : Compute total number of pairs per movies $C_i$, $C_j$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# movie_pair_counts_df Schema\n",
        "# root\n",
        "#  |-- movieId: integer (nullable = true)\n",
        "#  |-- count: long (nullable = true)\n",
        "\n",
        "movie_pair_counts_df = #TODO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3 : compute $\\widehat{PMI}(i, j)$\n",
        "\n",
        "Using the movie counts and the pair counts, compute the PMI dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pmi_df Schema\n",
        "# root\n",
        "#  |-- movieId1: integer (nullable = true)\n",
        "#  |-- movieId2: integer (nullable = true)\n",
        "#  |-- pmi: double (nullable = true)\n",
        "\n",
        "pmi_df = #TODO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Factorizing the PMI matrix with RSVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to create a mapping between movie ids and position in the matrix that we call vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_ordering_window = Window().orderBy(F.col(\"count\").desc())\n",
        "vocabulary_df = movie_counts_df.select(\n",
        "    \"movieId\",\n",
        "    (F.row_number().over(count_ordering_window) - F.lit(1)).alias(\"index\"),  # row number starts at 1\n",
        ").cache()\n",
        "\n",
        "vocabulary_df.show(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to build a scipy sparse matrix from the PMI dataframe. \n",
        "Thanks to our filtering, it is small enough to be collected into memory.\n",
        "\n",
        "Still this might take a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pmi_pdf = (\n",
        "    #TODO\n",
        "    .select(\"i\", \"j\", \"pmi\")\n",
        ").toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "pmi_matrix = coo_matrix((pmi_pdf[\"pmi\"].values, (pmi_pdf[\"i\"].values, pmi_pdf[\"j\"].values)))\n",
        "# We have built only the triangular matrix \n",
        "pmi_matrix = pmi_matrix + pmi_matrix.transpose()\n",
        "print(\n",
        "    f\"We have a sparse {pmi_matrix.shape} PMI matrix with {len(pmi_matrix.nonzero()[0]):,} entries\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the scikit-learn implementation of SVD https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html to factorize the PMI matrix. It uses the randomized SVD algorithm presented as a default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knn Index on movies embeddings from SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "movie_embeddings = svd.components_.transpose().astype('float32')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using the function defined above, compute the k nearest neighbor of a movie embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Faiss index version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "Matrix Factorization (énoncé)",
      "notebookOrigID": 4244742766502,
      "widgets": {}
    },
    "colab": {
      "collapsed_sections": [],
      "name": "td4-part1-matrix-factorization-questions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
